{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOvQhxcblf93VX1hrwQDkZ6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedAlshoki/InceptionModule/blob/main/Inception_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at1WHWokpxMo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ef48a64-47e3-4ae6-fabd-39267a505256"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_apJwxlLuVw-"
      },
      "source": [
        "pip install bayesian-optimization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tt6GlhwgrQk"
      },
      "source": [
        "import cv2 as cv\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers , regularizers , activations\n",
        "from bayes_opt import BayesianOptimization\n",
        "import numpy as np\n",
        "import tensorflow.keras.initializers\n",
        "import statistics\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import optimizers\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Define_Data():\n",
        "    X = []\n",
        "    Y = []\n",
        "\n",
        "    myfolder = '/content/drive/MyDrive/Graduation Project/Data_Augmented/'\n",
        "    My_Data= os.listdir(myfolder)\n",
        "    index = 0\n",
        "    for folder in My_Data:\n",
        "        s=r'/content/drive/MyDrive/Graduation Project/Data_Augmented/'+folder\n",
        "        for img in os.listdir(s):\n",
        "            image = cv.imread(os.path.join(s,img))\n",
        "            \n",
        "            if image is not None:\n",
        "                X.append(image)\n",
        "                Y.append(index)\n",
        "        index +=1\n",
        "        print(len(X))\n",
        "    \n",
        "    X_Train , X_Test , Y_Train , Y_Test = train_test_split(X,Y,test_size=0.25,shuffle=True) \n",
        "    X_Test = np.array(X_Test).reshape(len(X_Test),150,150,3)\n",
        "    X_Train=np.array(X_Train).reshape(len(X_Train),150,150,3)\n",
        "    Y_Train= np.array(Y_Train)\n",
        "    Y_Test = np.array(Y_Test)\n",
        "\n",
        "    #One Hit Encoding \n",
        "    onehot_encoder = OneHotEncoder(sparse=False)\n",
        "    Y_Train = Y_Train.reshape(len(Y_Train), 1)\n",
        "    Y_Test = Y_Test.reshape(len(Y_Test), 1)\n",
        "    \n",
        "    Y_Train = onehot_encoder.fit_transform(Y_Train)\n",
        "    Y_Test = onehot_encoder.fit_transform(Y_Test)\n",
        "    #scale\n",
        "    X_Train = X_Train.astype(\"float32\") / 255\n",
        "    X_Test = X_Test.astype(\"float32\") / 255\n",
        "    X_Train = tf.convert_to_tensor(X_Train, dtype=tf.float32)\n",
        "    X_Test = tf.convert_to_tensor(X_Test, dtype=tf.float32)\n",
        "    Y_Train = tf.convert_to_tensor(Y_Train, dtype=tf.float32)\n",
        "    Y_Test = tf.convert_to_tensor(Y_Test, dtype=tf.float32)\n",
        "    return X_Train , X_Test , Y_Train , Y_Test\n",
        "X_Train , X_Test , Y_Train , Y_Test = Define_Data()  "
      ],
      "metadata": {
        "id": "IEtWlokfRDEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inception_module(previous_layer,num_of_filters):\n",
        "    k = keras.backend.set_image_data_format('channels_last')\n",
        "    activation = activations.relu\n",
        "    #first layer 1*1\n",
        "    layer_1 = layers.Conv2D(num_of_filters[0],(1,1),padding = 'same',activation=activation)(previous_layer)\n",
        "    #second layer 3*3\n",
        "    layer_2 = layers.Conv2D(num_of_filters[1],(1,1),padding = 'same',activation=activation)(previous_layer)\n",
        "    layer_2_1 = layers.Conv2D(num_of_filters[2],(3,3),padding = 'same',activation=activation)(layer_2)\n",
        "    #third_layer Max Pooling\n",
        "    layer_3 = layers.MaxPool2D((2,2),strides= (1,1),padding='same')(previous_layer)\n",
        "    layer_3_1 = layers.Conv2D(num_of_filters[3],(1,1),padding='same',activation=activation)(layer_3)\n",
        "    #fourth layer 5*5\n",
        "    layer_4 = layers.Conv2D(num_of_filters[4],(1,1),padding = 'same',activation=activation)(previous_layer)\n",
        "    layer_4_1 = layers.Conv2D(num_of_filters[5],(5,5),padding = 'same',activation=activation)(layer_4)\n",
        "\n",
        "    #concatenate them all\n",
        "    final_layer = layers.concatenate([layer_1,layer_2_1,layer_3_1,layer_4_1], axis=3)\n",
        "    return final_layer"
      ],
      "metadata": {
        "id": "SaqX_G11Q6_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_model(number_of_inception_layers,drop_out):\n",
        "   activation = activations.relu\n",
        "   numper_of_filters_for_first_layer= [64,96,128,32,16,32]\n",
        "   Input_layer = layers.Input(shape=(150, 150, 3))\n",
        "   layer = layers.Conv2D(64, (7, 7), strides=1,padding='same', activation=activation)(Input_layer)\n",
        "   layer = layers.BatchNormalization()(layer)\n",
        "   MaxPooling = layers.MaxPool2D((2, 2), strides=2,padding='same')(layer)\n",
        "   layer = layers.Conv2D(192, (3, 3), strides=1,padding='same', activation=activation)(MaxPooling)\n",
        "   layer = layers.BatchNormalization()(layer)\n",
        "   layer = layers.MaxPool2D((3, 3), strides=2,padding='same')(layer)\n",
        "   for i in range(int(number_of_inception_layers)):\n",
        "       layer = inception_module(layer,numper_of_filters_for_first_layer)\n",
        "       layer = layers.BatchNormalization()(layer)\n",
        "       if i %2 == 0:\n",
        "           layer = inception_module(layer,numper_of_filters_for_first_layer)\n",
        "           layer = layers.BatchNormalization()(layer)\n",
        "           layer = layers.MaxPool2D((3, 3), strides=2,padding='same')(layer)\n",
        "           numper_of_filters_for_first_layer *=2\n",
        "       \n",
        "   Drop = layers.Dropout(drop_out)(layer)\n",
        "   Flat_layer = layers.Flatten()(Drop)\n",
        "   Hidden = layers.Dense(10, activation='relu',name=\"First_Hidden\")(Flat_layer)\n",
        "   Hidden = layers.Dense(3, activation='relu',name=\"Second_Hidden\")(Hidden)\n",
        "   #Hidden = layers.Dense(10, activation='relu',name=\"Last_Hidden\")(Hidden)\n",
        "   Output_layer = layers.Dense(units=3, activation='softmax',name=\"Final_Layer_1\") (Hidden)\n",
        "   model = keras.Model(inputs =Input_layer,outputs = Output_layer )\n",
        "   return model"
      ],
      "metadata": {
        "id": "5Zs-QJXcQxIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_the_model(number_of_inception_layers,drop_out,learning_rate):\n",
        "    model = generate_model(number_of_inception_layers,drop_out)\n",
        "    model.compile(loss= 'categorical_crossentropy', optimizer=optimizers.SGD(lr=learning_rate),metrics='accuracy')\n",
        "    monitor = EarlyStopping(monitor='accuracy',patience=50, mode='min', restore_best_weights=True)\n",
        "    model.fit(X_Train,Y_Train,callbacks=[monitor],epochs=280,verbose=0)\n",
        "    score = model.evaluate(X_Test, Y_Test)\n",
        "    tensorflow.keras.backend.clear_session()\n",
        "    return score[1]"
      ],
      "metadata": {
        "id": "yYz7kxEYQrvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pbounds = {'number_of_inception_layers' : (3 , 8),\n",
        "           'learning_rate': (0.0, 1.0),\n",
        "           'drop_out': (0.0, 0.499)\n",
        "          }\n",
        "optimizer = BayesianOptimization(f=evaluate_the_model,pbounds=pbounds,verbose=10)\n",
        "optimizer.maximize(init_points=10, n_iter=100,)\n",
        "print(optimizer.max)"
      ],
      "metadata": {
        "id": "J1lzKv4MQq6B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}